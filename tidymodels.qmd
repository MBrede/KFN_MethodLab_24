---
toc-title: '![](imgs/modeling_magpie.webp){width=240px}<br> <h3> tidymodels </h3>'
execute: 
  cache: true
---


# tidymodels {#sec-tidymodels}

Auch wenn der Workshop vielleicht bisher nicht den Eindruck gemacht hat - R ist natürlich vor allem als Sprache zur Vorbereitung und Durchführung von statistischen Analysen gedacht.

Die meisten grundlegenden Analysen lassen sich natürlich - genau wie das meiste der Inhalte die bisher Thema waren - mit `base`-R durchführen.

Im Kosmos des `tidyverse` hat sich aber auch hier eine Reihe an Paketen angesammelt, die viele Auswertungen streamlinen, leichter lesbarer machen und in einer gemeinsamen Sprache ausdrücken. Die unter dem Namen [`tidymodels`](https://www.tidymodels.org/packages/) gesammelten Pakete bieten dabei Helper für Inferenz-Statistik und Machine Learning-Anwendungen in der uns jetzt ja schon bekannten `tidy`-Schreibweise - also als eine Reihe von in pipelines verkett-baren Verben. Wie im `tidyverse` lässt sich auch diese Funktionssammlung mit einem Call laden:

```{r}
library(tidymodels)
```

Für schon routinierte Nutzer von R ist dabei die größte Umstellung, dass Modelle nicht mehr wie folgt in einer Zeile definiert werden. Im `datasets`-Paket wird der Datensatz `Sacramento` mitgeliefert, der Wohnungspreise in Sacramento beinhaltet:

```{r}
head(Sacramento)
```

Wenn wir in Base-R eine Regression der Wohnungsgröße und der Anzahl der Schlafzimmer auf den Preis durchführen wollen, sähe das wie folgt aus:

```{r}
lm(price ~ sqft + beds, data = Sacramento)
```

Und um inferenzstatistische Kennwerte zu erhalten könnten wir zum Beispiel `summary` aufrufen:

```{r}
lm(price ~ sqft + beds, data = Sacramento) %>% 
  summary()
```

Die Logik von `tidymodels` ist hierbei näher an ggplot2. Wir initiieren ein Modell aus der Familie, das wir fitten wollen und geben dann an, auf welche Daten wir dieses fitten wollen. Dazu übergeben wir die erstellte Modell-Instanz an die `fit`-Funktion mit den gesetzten Parametern:

```{r}
lm_model <- linear_reg()

lm_fit <- lm_model %>% 
  fit(price ~ sqft + beds, data = Sacramento)

lm_fit
```

Das Ergebnis ließe sich wieder mit `summary` zusammenfassen, in `tidymodels` gibt es dafür aber das Paket `broom` mit ein paar netteren Wrappern:

```{r}
lm_fit %>% 
  tidy()

lm_fit %>% 
  glance()
```


Der Schritt der anfänglichen Modelldefinition scheint erstmal mehr Aufwand zu sein - der Vorteil wird aber deutlich, wenn ich mich entscheide eine andere Modell-Architektur zur Lösung der Regression nutzen möchte.

Um statt einer least-squares-Regression eine regularisiertes Regressionsmodell einzusetzen, da ich befürchte dass Anzahl der Schlafzimmer und Größe der Wohnung korreliert sind, muss ich nur eine Kleinigkeit am Modell ändern:

```{r}
glmnet_model <- linear_reg(penalty = 0.95, mixture = 0.5) %>% 
  set_engine('glmnet')


glmnet_fit <- glmnet_model %>% 
  fit(price ~ sqft + beds, data = Sacramento)

glmnet_fit %>% 
  tidy()
```




Oder aber ich überlege mir, dass die Zusammenhänge sicher deutlich komplizierter sind und sich das Problem nur mit einem Deep-Learning-Ansatz^[wenn man einen Hidden Layer mit einem Neuron denn Deep nennen kann] lösen lässt - also nehme ich Keras als engine:
```{r}
keras_model <- linear_reg(penalty = 0.01) %>% 
  set_engine('keras')


keras_fit <- keras_model %>% 
  fit(price ~ sqft + beds, data = Sacramento)

keras_fit
```

Aber wenn ich schon bei Keras bin, warum dann nicht mit einem größeren Layer? Und warum lineare Aktivierung, wenn ich schon ein Deep-Learning-Backend bemühe?

```{r}
keras_model <- linear_reg(penalty = 0.01) %>% 
  set_engine('keras', hidden_units = 128, act = 'relu')


keras_fit <- keras_model %>% 
  fit(price ~ sqft + beds, data = Sacramento)

keras_fit
```

Je nach Modell sind die daraus resultierenden, interpretier- und testbaren Parameter natürlich andere.


## Modelltypen, Modes und Engines

Die Modelldefinition funktioniert in `tidymodels` über das `parsnip`-Paket. Die Syntax besteht auf der Modellseite dabei immer aus einem *Modelltyp*, im Beispiel `linear_reg`. 

Dabei gibt es für so gut wie jeden Anwendungsfall von statistischer oder ML-Modellierung einen Modelltyp, der das Problem abzubilden versucht. ^[Im mit `parsnip` gelieferten Datensatz `model_db` ist eine (so weit ich das richtig sehe) nicht vollständige Liste der möglichen Modelle - hier werden allein 37 verschiedene Modelle gelistet.]

Ein anderes Beispiel für einen Modelltypen ist ein `rand_forest`. Um unser Preis-Problem zu lösen könnten wir den Aufruf wie folgt gestalten: ^[Für die partykit-engine sind die Zusatzpakete `partykit` und `bonsai` nötig.]


```{r}
library(bonsai)
partykit_model <- rand_forest(trees = 500) %>% 
  set_mode('regression') %>% 
  set_engine('partykit')


partykit_fit <- partykit_model %>% 
  fit(price ~ sqft + beds, data = Sacramento)

partykit_fit$fit %>% 
  partykit::varimp()
```

Random Forests können sowohl Klassifikations- als auch Regressions-Probleme lösen. Mit dem Zusatz `set_mode` zur Pipeline können wir deshalb spezifizieren, welche Art von Problem mit dem Modell gefittet werden soll.

Nach Definition von Modelltyp und mode folgt eine *engine*, mit der das Problem gelöst werden soll.
In den Beispielen waren die engines erst `lm` aus `stats`, dann `glmnet` aus dem gleichnamigen Paket und abschließend der Wrapper `keras_mlp` um ein Keras-Netz aus dem `parsnip`-Paket. 
Zuletzt kam jetzt noch ein `cforest` aus dem `partykit` dazu.

`parsnip` macht mit den Anweisungen nichts anderes, als daraus einen dem Paket angemessenen Template-Call zu formulieren.
Diesen template-Call kann man sich exemplarisch für das zweite Beispiel wie folgt angucken:

```{r}
glmnet_model %>% 
  translate()
```

Für x, y und weights werden Platzhalter eingefügt, die dann im `fit`-Aufruf aufgefüllt werden.

Im Output sieht man auch, welche Werte für die so genannten *Main*-Argumente gesetzt sind.
`parsnip` unterscheidet nämlich zwischen denjenigen Parametern, die für viele oder gar alle einem Modelltyp angehörigen Modelle nötig sind und vereinheitlicht deren Aufruf. Diese verpflichtenden Modellparameter, z.B. der Grad der Regularisierung bei der linearen Regression, oder auch die Anzahl der zu trainierenden Bäume in einem Random Forest, werden dann direkt im ersten Modell-Call angegeben.

Dann gibt es aber noch für jede engine spezifische Argumente, deren Setzen für einen Großteil der anderen engines keinen Sinn ergäbe.
Diese *Engine-arguments* können im `set_engine`-Call übergeben werden. So können wir unserer `glmnet`-Regression zum Beispiel die (zumindest diskutable) Anweisung auf den Weg geben, die Mietpreise als poisson-verteilt anzunehmen:

```{r}
glmnet_model <- linear_reg(penalty = 0.95, mixture = 0.5) %>% 
  set_engine('glmnet', family = 'poisson')


glmnet_fit <- glmnet_model %>% 
  fit(price ~ sqft + beds, data = Sacramento)

tidy(glmnet_fit)
```

Alle standardmäßig implementierten möglichen Kombinationen von Modelltypen, Engines und Modes können in [dieser](https://www.tidymodels.org/find/parsnip/) Tabelle gefunden werden.

### Aufgabe

Für diese Aufgaben benötigen Sie wieder den Datensatz der Worldbank, der Import lief so:

```{r}
#| collapse: true
library(readxl)
worldbank_indicators <- read_excel("data/worldbank_indicators.xlsx")
```

Filtern Sie den Datensatz so, dass nur das Jahr 2019 vorliegt.

Fitten Sie die Daten mit zwei linearen Regressionsmodellen, einmal mit `lm` und einmal mit `glm` als engine. Setzen Sie für das `quasi`-Modell die family auf "quasi".
Dabei soll in beiden Fällen die mittlere Lebenserwartung als Kriterium mit dem Alkoholkonsum, dem GDP und dem Zugang zu Elektrizität vorhergesagt werden.

:::{class="card"}
:::{class="content"}
```{r}
fit_data <- worldbank_indicators %>% 
  filter(Year == 2019) %>% 
  select(Lebenserwartung = `Life expectancy at birth, total (years)`,
        GDP = `GDP per capita (current US$)`,
        Zugang = `Access to electricity (% of population)`,
        Alkoholkonsum = `Total alcohol consumption per capita (liters of pure alcohol, projected estimates, 15+ years of age)`)

lm_model <- linear_reg()

lm_fit <- lm_model %>% 
  fit(Lebenserwartung ~ Zugang + GDP + Alkoholkonsum, data = fit_data)

tidy(lm_fit)


glm_model <- linear_reg() %>% 
  set_engine('glm', family = 'quasi')

glm_fit <- glm_model %>% 
  fit(Lebenserwartung ~ Zugang + GDP + Alkoholkonsum, data = fit_data)

tidy(glm_fit)
```
:::
:::{class="overlay"}
Antwort aufdecken
:::
:::

Fitten Sie die Daten mit einem Random Forest mit `randomForest` als engine und 200 gefitteten Bäumen.

:::{class="card"}
:::{class="content"}
```{r}
rf_model <- rand_forest(trees = 200) %>% 
  set_mode('regression') %>% 
  set_engine('randomForest')

rf_fit <- rf_model %>% 
  fit(Lebenserwartung ~ Zugang + GDP + Alkoholkonsum, data = fit_data)

rf_fit
```

:::
:::{class="overlay"}
Antwort aufdecken
:::
:::


Fügen Sie mit `predict(<Ihr Modell-fit>, new_data=<Datensatz>)` und `mutate` drei Spalten an Ihren Datensatz an, in denen die jeweiligen Modellvorhersagen angegeben sind. Eventuell müssen Sie das Ergebnis der `predict`-Funktion mit `unlist` in einen Vektor überführen.

:::{class="card"}
:::{class="content"}
```{r}
fit_data <- fit_data %>% 
  mutate(lm = unlist(predict(lm_fit, fit_data)),
         glm = unlist(predict(glm_fit, fit_data)),
         rf = unlist(predict(rf_fit, fit_data)))
```

:::
:::{class="overlay"}
Antwort aufdecken
:::
:::


Pivotieren Sie den Datensatz ins long-Format, so dass alle Prognosen in einer Spalte vorliegen.

:::{class="card"}
:::{class="content"}
```{r}
fit_data <- fit_data %>% 
  pivot_longer(lm:rf,
               values_to = 'Prognose',
               names_to = 'Model')
```

:::
:::{class="overlay"}
Antwort aufdecken
:::
:::


Erstellen Sie einen ggplot, der auf der x-Achse die Prognosen und auf der y-Achse die tatsächlichen Werte abträgt. Färben Sie die Punkte nach dem GDP ein, skalieren Sie auc hdie Größe der Punkte nach dem GDP. Facettieren Sie den Plot nach den drei Modellen. Fügen Sie der Grafik mit `geom_abline` eine schwarze Linie hinzu, die mit einer Steigung von 1 und einem Intercept von 0 eine Diagonale einzeichnet.

:::{class="card"}
:::{class="content"}
```{r}
fit_data %>% 
  ggplot(aes(x = Prognose, y = Lebenserwartung, color = GDP, size = GDP)) +
  geom_abline(intercept = 0, slope = 1) +
  geom_point() +
  facet_wrap(~Model) 
```

:::
:::{class="overlay"}
Antwort aufdecken
:::
:::


## Recipes und Workflows

### Recipes

Neben dem Erstellen der Modelle bietet `tidymodels` auch pipeline-Interfaces für das standardisieren von der Vor- und Nachbereitung von Analysen.
Das Vorbereiten können dabei relativ aufwändige Aufgaben des feature-Engineerings wie Methoden zur Dimensionsreduktion sein, aber auch für den inferenzstatistischen Alltag relevantere Schritte wie die Logarithmierung oder Dummyfizierung von Variablen. 

Das dafür genutzte Paket heißt passenderweise `recipes`. Wenn wir in unserem Wohnungs-Beispiel vor der Analyse 
die Prädiktoren standardisieren wollen, fangen wir mit der Definition eines Rezeptes an:

```{r}
my_recipe <- recipe(price ~ sqft + beds,
                    data = Sacramento)
```

Das Rezept können wir nun schrittweise ergänzen. Wenn alle Prädiktoren und vielleicht auch das Kriterium standardisieren wollen, müssen wir zuerst die Daten zentrieren und dann skalieren.

Diese Schritte sind in `recipes` mit den angemessen als `step`-Funktionen benannten Anweisungen implementiert. Für die Schritte muss jeweils angegeben werden, auf welche Teile der Designmatrix die Operationen angewandt werden sollen.

Dazu können wir die schon bekannten tidy-select-helper nutzen, wir können aber auch die sepzifischen, von `recipes` gelieferten Auswahl-Helfer genutzt werden. Auf der mit `?selections` aufrufbaren Hifleseite sind alle aufgelistet.

Wir wollen wie gesagt alle numerischen Variablen z-transformieren, ergänzen unser Rezept also wie folgt:

```{r}
my_recipe <- recipe(price ~ sqft + beds,
                    data = Sacramento) %>% 
  step_center(all_numeric()) %>% 
  step_scale(all_numeric())
```

Wenn wir das Recipe aufrufen, wird uns eine Zusammenfassung unserer Vorbereitungsschritte angezeigt:

```{r}
my_recipe
```

Neben diesen Einfachen steps gibt es natürlich auch wesentlich kompliziertere, unter [diesem Link](https://www.tidymodels.org/find/recipes/) sind alle implementierten Steps aufgelistet.

Die für das Rezept nötigen "Zutaten" können wir uns mit `prep` vorbereiten lassen:

```{r}
my_prep <- my_recipe %>% 
  prep(Sacramento)

my_prep
```

Und mit `bake` können wir das Rezept abschließend anwenden:

```{r}
scaled_data <- my_prep %>% 
  bake(Sacramento)

scaled_data
```

Diesen Datensatz können wir dann wieder nutzen, um eine Regression zu fitten:

```{r}
lm_model <- linear_reg() 


lm_fit <- lm_model %>% 
  fit(price ~ sqft + beds, data = scaled_data)

lm_fit %>% 
  tidy()
```

### Workflows

Rezepte, Modelldefinition und -fit lassen sich im `tidymodels`-framework auch zu einer pipeline zusammenfügen.

Wie bei `parsnip` und `recipes` beginnt die Definition eines sogenannten Workflows aus `workflows` mit einer Objektdefinition.

```{r}
my_wf <- workflow()
```

Diesem Workflow können wir dann Rezept und Modell hinzufügen:

```{r}
my_wf <- my_wf %>% 
  add_recipe(my_recipe) %>% 
  add_model(lm_model)
```

Anschließend können wir den ganzen Workflow fitten:

```{r}
my_wf %>% 
  fit(data = Sacramento) %>% 
  tidy()
```


> Wozu ist das jetzt nützlich?

Nützlich wird das, wenn wir eine Analyse z.B. auf mehreren Datensätzen durchführen wollen.

Oder einen Workflow einmal mit und einmal ohne Logarithmierung des Kriteriums ausprobieren wollen.

Mit `update_recipe` können wir das ganz einfach testen:

```{r}
my_wf %>%
  update_recipe(my_recipe %>%  step_log(all_outcomes())) %>% 
  fit(data = Sacramento) %>% 
  tidy()
```

Außerdem lässt sich natürlich das Modell austauschen, wenn ich das möchte:

```{r}
my_wf %>%
  update_model(keras_model) %>% 
  fit(data = Sacramento)
```

### Aufgabe

Sie benötigen wieder den Datensatz der Worldbank. Erstellen Sie damit zwei Datensätze, einen mit den Daten zum Jahr 2019 und einen zum Jahr 2015.
Beide Datensätze sollen die Gesamt-Lebenserwartung als Spalte mit dem Namen "target" enthalten, außerdem alle Variablen mit Ausnahme der Jahreszahl und der spezifischen Lebenserwartungen.

:::{class="card"}
:::{class="content"}
```{r}
worldbank_indicators <- read_excel("data/worldbank_indicators.xlsx")

fit_data <- worldbank_indicators %>% 
  select(target = `Life expectancy at birth, total (years)`,
         everything(),
         - matches('ale'))

fit_data_2015 <- fit_data %>% 
  filter(Year == 2015) %>% 
  select(-Year)

fit_data_2019 <- fit_data %>% 
  filter(Year == 2019) %>% 
  select(-Year)
```

:::
:::{class="overlay"}
Antwort aufdecken
:::
:::


Erstellen Sie einen Workflow, mit dem für alle numerische Variablen alle fehlenden Werte durch die Spalten-Mittelwerte ersetzt werden. Außerdem sollen alle numerischen Werte auf einen Wertebereich von 0 bis 1 skaliert werden.
Die [Liste aller Steps](https://www.tidymodels.org/find/recipes/) kann Ihnen dabei helfen. 
Entfernen Sie außerdem mit `step_zv` alle Spalten, die nur einen Wert enthalten.
Der Workflow soll dann alle Variablen als Kriterium nutzen um die Variable `target` vorherzusagen. Anfänglich soll der Workflow eine lm-engine dazu nutzen.

:::{class="card"}
:::{class="content"}
```{r}
my_recipe <- recipe(target ~ ., data = fit_data_2015) %>% 
  step_impute_mean(all_numeric_predictors()) %>% 
  step_range(all_numeric_predictors()) %>% 
  step_zv(all_predictors())

lm_model <- linear_reg() 

my_wf <- workflow() %>% 
  add_recipe(my_recipe) %>% 
  add_model(lm_model)
```

:::
:::{class="overlay"}
Antwort aufdecken
:::
:::


Fitten Sie den Workflow auf die Datensätze zu den Jahren 2015 und 2019.

:::{class="card"}
:::{class="content"}
```{r}
lm_fit_2015 <- my_wf %>% 
  fit(data = fit_data_2015)

lm_fit_2019 <- my_wf %>% 
  fit(data = fit_data_2019)
```

:::
:::{class="overlay"}
Antwort aufdecken
:::
:::

Tauschen Sie das Modell gegen einen randomForest mit Regressions-Mode aus und fitten Sie wieder beide Modelle.

:::{class="card"}
:::{class="content"}
```{r}
rf_model <- rand_forest() %>% 
  set_mode('regression') %>% 
  set_engine('randomForest')

my_wf <- my_wf %>% 
  update_model(rf_model)

rf_fit_2015 <- my_wf %>% 
  fit(data = fit_data_2015)

rf_fit_2019 <- my_wf %>% 
  fit(data = fit_data_2019)
```
:::
:::{class="overlay"}
Antwort aufdecken
:::
:::

Erstellen Sie einen Datensatz mit allen Prognosen und targets. Pivotieren Sie diesen So, dass die targets in einer und die Prognosen in einer zweiten Spalte stehen. Nutzen Sie dazu zuerst das `names_pattern`-Argument der `pivot_longer`-Funktion und den `.value`-Platzhalter des `names_to`-Arguments. Anschließend müssen Sie vielleicht ein zweites Mal pivotieren.
Lesen Sie die Hilfeseite der `mape`-Funktion aus dem `yardstick` Paket und nutzen Sie diese um die Modelle oberflächlich zu vergleichen. 

:::{class="card"}
:::{class="content"}
```{r}
results = tibble(target_2015 = fit_data_2015$target,
                 target_2019 = fit_data_2019$target,
                 lm_2015 = unlist(predict(lm_fit_2015, fit_data_2015)),
                 lm_2019 = unlist(predict(lm_fit_2019, fit_data_2019)),
                 rf_2015 = unlist(predict(rf_fit_2015, fit_data_2015)),
                 rf_2019 = unlist(predict(rf_fit_2019, fit_data_2019))) %>% 
  pivot_longer(everything(),
               names_pattern = '(.+)_(.+)',
               names_to = c('.value', 'year')) %>% 
  pivot_longer(c(lm, rf))

results %>% 
  group_by(name) %>% 
  mape(target, value)
```

:::
:::{class="overlay"}
Antwort aufdecken
:::
:::


## Inferenzstatistik-Pipelines mit infer

Neben der Kombination von `parsnip` und `broom` bietet `tidymodels` mit `infer` ein weiteres Paket für den inferenzstatistischen Einsatz.

`infer` stellt dabei ein einfaches Interface zur Formulierung von Hypothesen und deren Testung mit Bootstrapping, Permutationstests und anderen Randomisierungsverfahren zur Verfügung.

Der Ablauf sieht dabei immer wie in @fig-infer aus. Zuerst wird/werden mit `specify` die Variablen spezifiziert, die von Interesse sind. Daraufhin wird mit `hypothesize` in Textform angegeben, welche Nullhypothese man über diese Variablen testen will. Mit `generate` werden anschließend Daten generiert, die dieser Nullhypothese entsprechen. Abschließend kann mit `calculate` aus den generierten Werten eine Verteilung berechnet werden, die mit den beobachteten Werten verglichen werden kann.

![Infer-Workflow - von [https://infer.tidymodels.org/](https://infer.tidymodels.org/)](https://raw.githubusercontent.com/tidymodels/infer/main/figs/ht-diagram.png){#fig-infer}

Wir könnten zum Beispiel die Annahme haben, dass die Art der Mietwohnungen unterschiedlich in den verschiedenen Städten verteilt sind. 
Die uns interessierenden Variablen sind also `type` und `city`. Das legen wir zuallererst mit `specify` fest:

```{r}
Sacramento %>% 
  specify(type ~ city)
```

Unsere Nullhypothese ist unabhängigkeit, die können wir also direkt hinzufügen:

```{r}
Sacramento %>% 
  specify(type ~ city) %>% 
  hypothesize('independence')
```

Um nun Daten für den Test zu generieren, können wir 1000 Datensätze mit Permuationen der Reihenfolge einer der Variablen generieren:

```{r}
Sacramento %>% 
  specify(type ~ city) %>% 
  hypothesize('independence') %>% 
  generate(reps = 1000, type = 'permute')
```

Zuletzt müssen wir noch eine Teststatistik berechnen, deren Verteilung wir betrachten können.
Bei unserem Fall bietet sich natürlich $\chi^2$ an:

```{r}
Sacramento %>% 
  specify(type ~ city) %>% 
  hypothesize('independence') %>% 
  generate(reps = 1000, type = 'permute') %>% 
  calculate('Chisq')
```

Die Verteilung können wir jetzt mit mitgelieferten `ggplot2`-Wrappern darstellen:

```{r}
Sacramento %>% 
  specify(type ~ city) %>% 
  hypothesize('independence') %>% 
  generate(reps = 1000, type = 'permute') %>% 
  calculate('Chisq') %>% 
  visualize()
```

Und wenn wir die beobachtete Teststatistik berechnen, können wir auch eine Signifikanz-Aussage tätigen:

```{r}

obs_chisq <- Sacramento %>% 
  specify(type ~ city) %>% 
  calculate('Chisq')
  
Sacramento %>% 
  specify(type ~ city) %>% 
  hypothesize('independence') %>% 
  generate(reps = 1000, type = 'permute') %>% 
  calculate('Chisq') %>% 
  visualize() +
  shade_p_value(obs_stat = obs_chisq, direction = "greater")
```


### Aufgabe 
Für diese Aufgabe benötigen Sie den `attrition`-Datensatz, der mit `tidymodels` geliefert und geladen wird.

Verschaffen Sie sich einen Überblick über den Datensatz. Wir benötigen die Variable `Gender` und die Variable `HourlyRate`.

Stellen Sie eine `specify` -> `hypothesize` -> `generate`-Pipeline für einen t-Test auf den Unterschied in `HourlyRate` zwischen den Auspärgungen von `Gender` auf. Lesen Sie die Hilfeseiten von `hypothesize` und `generate` um den richtigen Begriff für die angemessene Nullhypothese und das angemessene Sampling-Vorgehen zu bestimmen.

:::{class="card"}
:::{class="content"}
```{r}
hypothesis <- attrition %>% 
  specify(HourlyRate ~ Gender) %>% 
  hypothesize('independence') %>% 
  generate(reps = 1000, type = 'bootstrap')
```

:::
:::{class="overlay"}
Antwort aufdecken
:::
:::

Berechnen Sie mit `calculate` den empirischen t-Wert und vergleichen Sie ihn mit `visualize` mit der simulierten Verteilung.

:::{class="card"}
:::{class="content"}
```{r}
simulated <- attrition %>% 
  specify(HourlyRate ~ Gender) %>% 
  hypothesize('independence') %>% 
  generate(reps = 1000, type = 'permute') %>% 
  calculate('t', order = c('Male', 'Female'))

observed <- attrition %>% 
  specify(HourlyRate ~ Gender) %>% 
  calculate('t', order = c('Male', 'Female'))

simulated %>% 
  visualize() +
  shade_p_value(obs_stat = observed, direction = "two.sided")
```
:::
:::{class="overlay"}
Antwort aufdecken
:::
:::
